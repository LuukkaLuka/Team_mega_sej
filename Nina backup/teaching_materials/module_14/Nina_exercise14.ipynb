{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Session 14: Text as Data\n",
    "\n",
    "In session 14 you will learn how to preprocess text data and structure it, so we can exploit the information in the texts. \n",
    "\n",
    "You will work on a larger exercise where you will use the tools you learn. In the exercise you will use the logistic regression model, which is fitted on the movie review dataset in the slides to predict sentiments on a completely different dataset: \n",
    "\n",
    "- The dataset contains tweets about US airlines. You can read more about it here: https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment?resource=download\n",
    "- Each tweet has been classified as positive, negative or neutral, which makes it possible for us to compare the predictions from our logistic regression model with the actual sentiments.\n",
    "\n",
    "The purpose of this exercise is to learn the pitfalls of fitting a model to one kind of text data, and then use the model to predict sentiments of another kind of text data (*cross-domain evaluation*). I.e., we will investigate how *generalizable* our model is.\n",
    "\n",
    "- Spoiler: The words in the tweets convey a completely different meaning than the words in the movie reviews, so our movie review logistic regression model is not good at predicting sentiments in tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the logistic regression model on the movie review data\n",
    "\n",
    "Before we can get started, you need to run the code below that fits a logistic regression model on the movie review data.\n",
    "\n",
    "We will then use that model to predict sentiments in the twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "# Clean reviews\n",
    "def cleaner(document):\n",
    "    document = document.lower() #To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document) #Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s]','', document) #Remove non-alphanumeric characters\n",
    "    return document\n",
    "\n",
    "df['review'] = df['review'].apply(cleaner)\n",
    "\n",
    "# Load train and test sets to different dataframes\n",
    "df_train = df[df.set==\"train\"]\n",
    "df_test = df[df.set==\"test\"]\n",
    "\n",
    "# Sort the data randomly to mix positive and negative reviews\n",
    "np.random.seed(0)\n",
    "df_train = df.reindex(np.random.permutation(df_train.index))\n",
    "df_test = df.reindex(np.random.permutation(df_test.index))\n",
    "\n",
    "# Take out X and Y variable\n",
    "x_train = df_train['review'].values\n",
    "x_test = df_test['review'].values\n",
    "y_train = df_train['sentiment'].values\n",
    "y_test = df_test['sentiment'].values\n",
    "\n",
    "# Make our bag of words\n",
    "tfidf = TfidfVectorizer()\n",
    "x_train_bag = tfidf.fit_transform(x_train)\n",
    "\n",
    "# Fit the model\n",
    "lr_reviews = LogisticRegression(random_state=0) #Text classifier\n",
    "lr_reviews.fit(x_train_bag,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHLDsg-Cvo8o"
   },
   "source": [
    "# Part 1: Cross-domain evaluation of a logistic regression model fitted on movie review data\n",
    "\n",
    "First load the twitter data, which you can find on github under module 14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv(\"AirlineTweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, change the sentiment to \"positive\" if the sentiment is labelled \"neutral\" to have only two categories. In the same time store the sentiments in a list for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiments = [0 if i==\"negative\" else 1 for i in tweet_df.airline_sentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to watch the video below before moving on to the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('piawPVa2Zjk', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.1:** Preprocess the twitter texts. You should at least do the following:\n",
    "> - Make all letters lower case\n",
    "> - Remove mentions; i.e. \"@user\". You can do this with regex.\n",
    "> - Can you think of other things to clean? Take a look at some of the tweets to look for other unnecessary stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweet_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Load and preprocess Twitter data\n",
    "def preprocess_tweet(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'@user', '', text)  # Remove mentions\n",
    "    text = re.sub(r'<[^>]*>', ' ', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    # You can add more cleaning steps if necessary\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the 'tweet' column\n",
    "tweet_df['text'] = tweet_df['text'].apply(preprocess_tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to watch the video below before moving on to the next exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('UtyYHIDwN8A', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.2:** Make your bag of words from the tweets using the tf-idf vectorizer (\"tfidf\") previously fitted on the movie review data. \n",
    "\n",
    "> *Hint:* You should use the `transform()` method instead of the `fit_transform()` because you have already fitted the vocabulary of the bag of words on the movie review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'text' column from tweet_df\n",
    "x_tweets = tweet_df['text'].values\n",
    "\n",
    "# Transform the tweets using the previously fitted tfidf vectorizer\n",
    "x_tweets_bag = tfidf.transform(x_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to watch the video below before moving on to the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('qzSm3rSx2Iw', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.3:** Use the trained logistic regression model from above (\"lr_reviews\") to predict the sentiment of the tweets.\n",
    "> - Report testing accuracy\n",
    "\n",
    "> *Hint:* Use the \"tweet_sentiments\" list from above to compare the predicted sentiments and the actual sentiments and compute the testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment using the trained model\n",
    "y_pred = lr_reviews.predict(x_tweets_bag)\n",
    "\n",
    "# Compare predicted sentiments with actual sentiments and calculate accuracy\n",
    "correct_predictions = sum(1 for pred, actual in zip(y_pred, tweet_sentiments) if pred == actual)\n",
    "total_tweets = len(tweet_sentiments)\n",
    "accuracy = correct_predictions / total_tweets\n",
    "\n",
    "print(\"Testing Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.4:** How well does the logistic regression model from the review data perform in this other domain?\n",
    "> - Why do you think it does not perform as well?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here\n",
    "\n",
    "The logistic regression model trained on movie review data may not perform well in Twitter sentiment analysis due to language differences, distinct sentiment expressions, and context variations between movie reviews and tweets. The model's vocabulary might not align with Twitter-specific terms, and its training might not account for Twitter's noisy data, slang, and short text. Additionally, differences in sentiment distribution and feature relevance can contribute to the performance gap. Transferring models between domains often encounters these challenges, requiring domain-specific adjustments for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.5:** Now train a new logstic regression model on the twitter data:\n",
    "> 1. Fit a new bag of words on the twitter texts\n",
    "> 2. Fit a logistic regression model on the twitter bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a new bag of words on the twitter texts\n",
    "tfidf_tweets = TfidfVectorizer()  # Initialize a new TF-IDF vectorizer\n",
    "x_tweets_bag = tfidf_tweets.fit_transform(x_tweets)  # Fit and transform Twitter texts\n",
    "\n",
    "# Fit a logistic regression model on the twitter bag of words\n",
    "lr_tweets = LogisticRegression(random_state=0)  # Initialize a new logistic regression model\n",
    "lr_tweets.fit(x_tweets_bag, tweet_sentiments)  # Fit the model on the bag of words and sentiment labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.6:** What are the most important features/words (high and low coefficients) in the new model? \n",
    "> - Do they differ from the most important features from the old model?\n",
    "> - What does this mean for the models' ability to generalize to new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names from the TF-IDF vectorizer\n",
    "feature_names = tfidf_tweets.get_feature_names_out()\n",
    "\n",
    "# Get the coefficients of the logistic regression model\n",
    "coefficients = lr_tweets.coef_[0]\n",
    "\n",
    "# Create a dictionary to associate feature names with their coefficients\n",
    "feature_coefficients = dict(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the feature_coefficients dictionary by coefficient values\n",
    "sorted_features = sorted(feature_coefficients.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top and bottom features\n",
    "num_features = 10  # Number of features to display\n",
    "print(\"Top {} features:\".format(num_features))\n",
    "for feature, coef in sorted_features[:num_features]:\n",
    "    print(feature, coef)\n",
    "\n",
    "print(\"\\nBottom {} features:\".format(num_features))\n",
    "for feature, coef in sorted_features[-num_features:]:\n",
    "    print(feature, coef)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Social Data Science: Text as Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "604903271ebee04a1d41a6e5cf13d16aa8edab4556eed9bb8d54d40193dfe553"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
