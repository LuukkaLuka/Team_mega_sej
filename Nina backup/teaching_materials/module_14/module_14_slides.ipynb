{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xom1o-ebeBv3",
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Session 14: Text as Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Required readings\n",
    "\n",
    "- Gentzkow, M., Kelly, B.T. and Taddy, M., 2019. [\"Text as data\"](https://doi.org/10.1257/jel.20181020) *Journal of Economic Literature* 57(3).\n",
    "  - Following sections:\n",
    "    - 1. Introduction\n",
    "    - 2. Representing Text as Data\n",
    "\n",
    "- Chapter 2. Dan Jurafsky and James H. Martin: [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "  - Following sections:\n",
    "    - 2.4 Text Normalization\n",
    "\n",
    "- PML; Python Machine Learning, 3rd ed. (2019) by Sebastian Raschka & Vahid Mirjalili: following sections from chapter 8:\n",
    "  - Introduction\n",
    "  - Preparing the IMDb movie review data for text processing\n",
    "  - Introducing the bag-of-words model\n",
    "  - Training a logistic regression model for document classification\n",
    "  - Topic modeling with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of Session 14\n",
    "\n",
    "1. **Intro to text as data**\n",
    "2. **Examples of text as data for social scientists**\n",
    "3. **What is a text?**\n",
    "    - What do we mean by a \"document\"?\n",
    "    - We need to represent the words of a text in a structured way!\n",
    "4. **A text data analysis recipe**\n",
    "    1. Specify your document\n",
    "    2. Preprocess your text\n",
    "    3. Apply\n",
    "5. **Cleaning and preprocessing text**\n",
    "    - Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "    - Tokenization and stop-words\n",
    "    - Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "6. **Bag of Words model**\n",
    "    - Term frequency\n",
    "    - N-grams\n",
    "    - Term frequency - Inverse Document Frequency\n",
    "7. **Applications:**\n",
    "    1. **Training a logistic model to classify whether a text is positive or negative**\n",
    "        - IMDB reviews\n",
    "    2. **Lexicons**\n",
    "        - Is a word positive or negative?\n",
    "    3. **Topic modelling**\n",
    "        - Assign topics to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Intro to text as data\n",
    "\n",
    "Regard this session as an appetizer!\n",
    "- Text as data can be a course in itself\n",
    "- We cannot go into details, so don't worry if you do not understand everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use the session as an overview of what text analysis can do\n",
    "    - What do you find interesting? Dive into the details yourself\n",
    "    - Maybe already in the exam project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Want to work with text as data?\n",
    "    - Good starting point is PML chapter 8!\n",
    "        - Nice and easily accessible introduction to text data analysis\n",
    "        - Read it carefully\n",
    "            - There are many steps in text data analysis\n",
    "            - If you miss one step, the other steps might be hard to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Examples of text as data for social scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Examples you have already seen in the course\n",
    "\n",
    "- News paper articles\n",
    "- Job posts\n",
    "- Reviews on Trustpilot (quick example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Other examples\n",
    "\n",
    "- Social media (tweets, Facebook posts etc.)\n",
    "- Text from central bank reports: https://sekhansen.github.io/pdf_files/jme_2019.pdf \n",
    "- Text from annual reports: https://www.nationalbanken.dk/da/publikationer/Documents/2018/11/WP_130.pdf\n",
    "- Congressional speeches and partisanship in the US: https://scholar.harvard.edu/files/shapiro/files/politext.pdf \n",
    "- Property descriptions on property portals\n",
    "- AirBnB descriptions\n",
    "- Can you find more examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project ideas\n",
    "\n",
    "- Predicting election outcomes or market trends from sentiment\n",
    "- Stance or sentiment towards political parties\n",
    "- Hate speech detection\n",
    "- Analysing the most important topics in a public debate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. What is a text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## A dataset of movie reviews and sentiment towards the movies\n",
    "\n",
    "*(Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).*\n",
    "\n",
    "*Data from http://ai.stanford.edu/~amaas/data/sentiment/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So what is a text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "review = df['review'][1]\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can also call our text a **document**\n",
    "    - The document determines at which level we will analyse the text. For example, the text above can be analysed in different ways:\n",
    "        - split each sentence to analyse them separately: *each sentence* is then defined as a document\n",
    "        - analyse the whole text: *the whole text* is then defined as a document\n",
    "        - analyse all the reviews that the author has written: *all reviews combined* are then defined as a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which one is the right definition of the document?\n",
    "    - It depends on the task you would like to solve\n",
    "        - Are there any dependencies across the author's reviews? Then it might be a good idea to combine them all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does a document consist of?\n",
    "\n",
    "- WORDS!\n",
    "- In the raw text, words are not structured in any way\n",
    "    - We need structured data to analyse it!\n",
    "    - --> Structure the words in a Bag of Words model (more about that later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. A 'text as data' recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## A. Specify what is your document\n",
    "\n",
    "- Is it every single tweet?\n",
    "- Daily tweets?\n",
    "- Monthly tweets?\n",
    "- Or all tweets a person has ever made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## B. Preprocess the text: Reduce the number of language elements\n",
    "\n",
    "- Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "- Tokenization and stop-words\n",
    "- Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## C. Apply: What question would you like to answer and what is the right tool?\n",
    "\n",
    "- Machine learning model for sentiment analysis\n",
    "- Lexicons\n",
    "- Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video 14.1: Preprocessing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Preprocessing text data (second step in our recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Different steps in preprocessing:\n",
    "\n",
    "1. Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "2. Tokenization and stop-words\n",
    "3. Stemming and lemmatization\n",
    "\n",
    "Which preprocessing steps that are important depends on the problem you will solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Clean text: ignore casing, HTML markup, non-words\n",
    "\n",
    "- Casing: \n",
    "    - We want \"Movie\" and \"movie\" to be the same word, so we change all letters to lower case\n",
    "- HTML markup:\n",
    "    - In our review example we see there is some unwanted HTML markup left. We want to drop it\n",
    "- Non-words: \n",
    "    - Any other character than words or numbers (non-alphanumeric characters) are typically not important for text data analysis, so we may drop them\n",
    "    - Exceptions:\n",
    "        - Emoticons may very much give information about sentiment in a text\n",
    "        - Dollar signs to indicate a price. Punctuation to indicate decimals in the price\n",
    "    - It all depends on the problem you want to solve!\n",
    "    - Careful: You might not want to remove any non-alphanumeric characters before you tokenize (next step)\n",
    "- Other stuff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Change to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "review_low = review.lower()\n",
    "review_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Remove HTML markup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "review_noHTML = re.sub(r'<[^>]*>', ' ', review_low) #Regex pattern matches the HTML markup surrounded by \"<\" and \">\" and replace it with ' ' using the method sub()\n",
    "review_noHTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Remove all characters that are not words or numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "review_cleaned = re.sub(r'[^\\w\\s]','',review_noHTML) #Regex pattern matches any non-alphanumeric characters and replace them with '' using the method sub()\n",
    "review_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Other stuff?\n",
    "\n",
    "- There may be other things you need to remove before you are ready to move on\n",
    "- It depends on the texts you are dealing with and the problem you want to solve\n",
    "    - Investigate the texts\n",
    "    - Make sure that you keep all the important stuff and remove the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now apply our cleaning process on all reviews in the dataset to work with it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cleaner(document):\n",
    "    document = document.lower() #To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document) #Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s]','', document) #Remove non-alphanumeric characters\n",
    "    return document\n",
    "\n",
    "df['review'] = df['review'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Tokenization (I/II)\n",
    "\n",
    "- Tokenization is about splitting the document into meaningful elements (/*tokens*)\n",
    "    - Tokens can be thought of as words in a sentence or sentences in a text\n",
    "- Simplest tokenization: Split the cleaned document at its whitespaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split at whitespace with the split() method\n",
    "review_tokens = review_cleaned.split()\n",
    "review_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Tokenization (II/II)\n",
    "\n",
    "- The simple tokenization might not suffice in some cases:\n",
    "    - How should we treat abbreviations like Ph.D.? And dollar signs before a price? And punctuation that indicates decimals?\n",
    "- The NLTK library has some [tokenizer packages](https://www.nltk.org/api/nltk.tokenize.html) that can hep you:\n",
    "    - `word_tokenize()` splits the words \n",
    "    - If you have twitter data, then `TweetTokenizer()` will keep the hashtag intact\n",
    "    - You can also define your own tokenization pattern using regex with `regexp_tokenize()`\n",
    "- But in many cases it is just fine to use `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "review_tokens = nltk.tokenize.word_tokenize(review_cleaned)\n",
    "review_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stop-words:\n",
    "\n",
    "- Words that are extremely common in all texts\n",
    "- Probably bear no useful information about the text --> we want to remove them\n",
    "- Examples: *is, and, has, like...*\n",
    "\n",
    "Use the NLTK library of 127 English stop-words\n",
    "- NLTK (Natural Language ToolKit) is a popular Python package for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "review_nostop = [i for i in review_tokens if i not in stop]\n",
    "review_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(len(review_tokens))\n",
    "print(len(review_nostop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Stemming and lemmatization\n",
    "\n",
    "#### Stemming:\n",
    "- The process of transforming a word into its root form\n",
    "- Allows us to map related words to the same stem\n",
    "- Examples: `'runners', 'run', 'running'` becomes `'runner', 'run', 'run'`. `'wonderful'` becomes `'wonder'`.\n",
    "- You can use the Porter stemmer in the NLTK library to stem your words: `PorterStemmer()`\n",
    "    - With stemming we generally just remove the suffix of the word: very simple method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Stem the words\n",
    "porter = nltk.PorterStemmer()\n",
    "review_stemmed = [porter.stem(i) for i in review_nostop]\n",
    "review_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lemmatization:\n",
    "\n",
    "- Stemming can create non-real words in some cases (see above)\n",
    "- Lemmatization is more advanced and seeks to find the grammatically correct form of the word (the lemma)\n",
    "    - Example: `'coding', 'code', 'coded'` will all be lemmatized to `'code'`\n",
    "- Lemmatization demands a lot of computer power --> it is slow\n",
    "- In practice there are little difference between stemming and lemmatization on the performance of text classification\n",
    "    - [Influence of Word Normalization on Text Classification](https://www.researchgate.net/publication/250030718_Influence_of_Word_Normalization_on_Text_Classification)\n",
    "\n",
    "You can use the [WordNet](https://wordnet.princeton.edu/) lemmatizer from NLTK\n",
    "- WordNet is a large lexical database of English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatize the words with the WordNetLemmatizer\n",
    "nltk.download('omw-1.4') #Download OpenMultilingualWordnet\n",
    "nltk.download('wordnet')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "review_lemma = [wnl.lemmatize(i) for i in review_nostop]\n",
    "review_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video 14.2: The Bag of Words model and tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bag of Words model\n",
    "\n",
    "Read more about the bag of words model in this article: https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "- It can be a good starting point to go into more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To exploit the information in text data we need to structure it in some way\n",
    "    - Raw text is not structured\n",
    "- A simple way to structure the documents/texts is the Bag of Words model\n",
    "    - The Bag of Words model simply counts the number of times each word occurs in a document\n",
    "    - That way we can store all documents and word counts in one big matrix (a term-document frequency matrix):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A Bag of Words model:\n",
    "<img src=\"https://drive.google.com/uc?exportview&id=1-VxQqdWhzIVt5l_7W-WljUa_iY8euUFk\"/>\n",
    "\n",
    "- Each row represents a document, and each column represents a word\n",
    "- The values in the matrix are the count of each word in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can construct a bag of words with our review data using the module [feature_extraction](https://scikit-learn.org/stable/modules/feature_extraction.html) from the Scikit-learn library\n",
    "- The [CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class constructs the bag of words for us\n",
    "\n",
    "Let us first do it for the first two reviews:\n",
    "- The `fit_transform()` method in the CountVectorizer() class first finds all the words in the documents (learn the vocabulary), and then constructs the matrix (count the words in each document):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer() #Store the class in 'count' to ease coding\n",
    "\n",
    "review_array = df['review'].values[0:2] #Take the first two reviews and store them in an array\n",
    "bag = count.fit_transform(review_array) #fit_transform takes an array as input and outputs the bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see how the bag of words looks in the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "count_array = bag.toarray() #Make the bag to an array\n",
    "matrix = pd.DataFrame(data=count_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The number of times a word (/term) occurs in a document is also called the **term frequency**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## N-grams:\n",
    "\n",
    "- In our bag of words from above each term represent **one** word\n",
    "    - It is a bag of words model with **1-grams**\n",
    "- I.e., we pool all words from a document into one big bag\n",
    "    - --> we lose all information that lies in the order of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead we can specify for example 2-grams:\n",
    "    - With 1-grams: \"My name is Hjalte\" will yield the terms; 'My', 'name', 'is', 'Hjalte'\n",
    "    - With 2-grams: \"My name is Hjalte\" will yield the terms; 'My name', 'name is', 'is Hjalte'\n",
    "- N-grams of more than 1 is a way to keep some of the information in the order of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us see how to do it in Python:\n",
    "- You can choose the N-grams via the `ngram_range()` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "count = CountVectorizer(ngram_range=(2,2)) #Choose only 2-grams\n",
    "\n",
    "review_array = df['review'].values[0:2]\n",
    "bag = count.fit_transform(review_array)\n",
    "\n",
    "count_array = bag.toarray() #Make the bag to an array\n",
    "matrix = pd.DataFrame(data=count_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note: We will get more terms with N-grams of higher degrees\n",
    "- More terms makes the bag of words model more computationally heavy to work with\n",
    "- **Classic trade-off in text as data: Trade-off between information and computer power**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Term frequency-inverse document frequency\n",
    "\n",
    "- From the matrix above you can see that we very fast get a lot of terms even with few documents\n",
    "- It is a problem for the computational efficiency\n",
    "\n",
    "#### Is there a way to limit the terms that do not provide a lot of information?\n",
    "- The technique called: Term frequency-inverse document frequency!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Background:\n",
    "\n",
    "- When analyzing text data we often have words that appears frequently across many documents\n",
    "    - These words typically do not carry much information about each document --> they are simply just in all documents\n",
    "- Similarly there will be words that are very rare\n",
    "    - These words will carry a lot of information\n",
    "    - But the information they provide may not be enough to counteract the computational cost they carry \n",
    "--> We want to down-weight very common words and very rare words\n",
    "- That is what the term frequency - inverse document frequency (tf-idf) technique does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tf-idf:\n",
    "The tf-idf is computed like this:\n",
    "\n",
    "$tf-idf(t,d) = tf(t,d) \\times idf(t,d)$\n",
    "\n",
    "- $tf(t,d)$ is the term frequency and measures how many times a word/term $t$ occurs in a document $d$ (just as you have seen with the bag of words model)\n",
    "\n",
    "- $idf(t,d)$ is computed like this: $idf(t,d) = log \\frac{n_d}{1+df(t,d)}$\n",
    "\n",
    "    - $n_d$ is the total number of documents, and $df(t,d)$ is the number of documents $d$ that contains the term $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Very common words will have low tf-idf score because $idf(t,d)$ will be low\n",
    "- Very rare words will have low tf-idf score because $tf(t,d)$ will be low\n",
    "\n",
    "Common practice:\n",
    "- Only keep the words in a document if they have a tf-idf score above some threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we compute the tf-idf score in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer() #Ease coding\n",
    "bag_tfidf = tfidf.fit_transform(bag) #Compute the tf-idf score from the bag of words from before ('bag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_array = bag_tfidf.toarray() #Make the bag to an array\n",
    "matrix_tfidf = pd.DataFrame(data=tfidf_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "matrix_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video 14.3: Text as data applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Applications (third step in our recipe)\n",
    "\n",
    "- Training a logistic model to classify whether a text is positive or negative\n",
    "- Lexicons\n",
    "- Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Applications (I/III): Training a logistic model for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall the structure of our movie review dataset:\n",
    "- Variable containing the reviews ('review')\n",
    "- Variable stating whether the person had a positive or negative sentiment towards the movie ('sentiment')\n",
    "- Variable stating whether the review is in the test or train set ('set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have labelled each review with a sentiment\n",
    "\n",
    "- --> We can train a machine learning model on our \"train reviews\" to predict the sentiment of our \"test reviews\"\n",
    "    - I.e., the goal is to predict the sentiment (positive or negative) of the reviews just by inputting the words in the reviews\n",
    "    \n",
    "We will use a logistic regression model for this text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we do it in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, we load the train and test dataset into two different datasets:\n",
    "    - Remember that we have already cleaned the data with our cleaner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "df_train = df[df.set==\"train\"]\n",
    "df_test = df[df.set==\"test\"]\n",
    "\n",
    "# Sort the data randomly to mix positive and negative reviews\n",
    "np.random.seed(0)\n",
    "df_train = df.reindex(np.random.permutation(df_train.index))\n",
    "df_test = df.reindex(np.random.permutation(df_test.index))\n",
    "\n",
    "# Take out X and Y variable\n",
    "x_train = df_train['review'].values\n",
    "x_test = df_test['review'].values\n",
    "y_train = df_train['sentiment'].values\n",
    "y_test = df_test['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Second, we need to make our bag of words and down-weight common and rare words with tf-idf\n",
    "    - Remember we used `CountVectorizer` and `TfidfTransformer` to do this\n",
    "    - `TfidfVectorizer` combines the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "x_train_bag = tfidf.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Third, we fit our logistic regression model on the training set's bag of words (x_train_bag) and the true sentiments (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=0) #Text classifier\n",
    "lr.fit(x_train_bag,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fourth, we can now test our fitted logistic regression model on both the train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to make a tf-idf bag of words for the test set as well.\n",
    "# (use the transform() method for that: do NOT use fit_transform() as in the train set. Because we only use the words from the train set to fit our model on)\n",
    "x_test_bag = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Then we predict the sentiment \n",
    "train_preds = lr.predict(x_train_bag)\n",
    "test_preds = lr.predict(x_test_bag)\n",
    "\n",
    "# And we compare the predicted sentiment with the actual sentiment\n",
    "print(\"Training accuracy:\", np.mean([(train_preds==y_train)]))\n",
    "print(\"Testing accuracy:\", np.mean([(test_preds==y_test)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can use the coefficients from the fitted model to say something about the importance of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the words (features)\n",
    "features = ['_'.join(s.split()) for s in tfidf.get_feature_names_out()]\n",
    "\n",
    "# Get the coefficients from the fitted model\n",
    "coefficients = lr.coef_\n",
    "\n",
    "# Present coefficients for each feature\n",
    "coefs_df = pd.DataFrame.from_records(coefficients, columns=features)\n",
    "coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Print the 20 words with highest positive sentiment\n",
    "print(coefs_df.T.sort_values(by=[0], ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Print the 20 words with lowest positive sentiment\n",
    "print(coefs_df.T.sort_values(by=[0], ascending=True).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Applications (II/III): Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sometimes we do not have labelled data as in our IMDB reviews example\n",
    "\n",
    "- I.e., we do not know in advance whether a review has a positive or negative sentiment towards a movie\n",
    "    - Recall: For each review we had a variable called 'sentiment' which stated whether the person writing the review had a positive or negative sentiment towards the movie\n",
    "- Then we cannot train a machine learning to classify the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead, we can use predefined lexicons!\n",
    "\n",
    "- The lexicons have a dictionary of words that can have some predefined labels:\n",
    "    - polarity score: positive, negative or neutral sentiment\n",
    "    - mood\n",
    "    - and so on\n",
    "\n",
    "We can use these predefined labels to score the sentiment of texts\n",
    "- The more positive words in the text, the more positive will the sentiment be\n",
    "- The more negative words in the text, the more negative will the sentiment be\n",
    "\n",
    "You can read more about lexicons [here](https://medium.com/nerd-for-tech/sentiment-analysis-lexicon-models-vs-machine-learning-b6e3af8fe746) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different lexicons:\n",
    "\n",
    "- AFINN: https://github.com/fnielsen/afinn\n",
    "- VADER: https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### AFINN:\n",
    "\n",
    "- Danish lexicon\n",
    "- Simple and popular lexicon\n",
    "- Word-list based: Contains 3382 words that are scored for polarity\n",
    "\n",
    "Positive score: Positive sentiment. Negative score: Negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### VADER:\n",
    "\n",
    "- Specifically tuned to social media\n",
    "- VADER scores both polarity and intensity of emotion\n",
    "- Word-list based as AFINN\n",
    "- But also rule-based:\n",
    "    - Example: It knows that \"dit not love\" is negative because of the negation\n",
    "    \n",
    "Positive score: Positive sentiment. Negative score: Negative sentiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it work in practice?\n",
    "\n",
    "- The document is tokenized (as you know how to do now)\n",
    "- Each token in the document is matched with the words in the lexicon: Are they positive, negative or neutral?\n",
    "- All the token sentiment scores in the document are summed or averaged to predict the overall sentiment of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it work in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True) #Also use the emoticons in the lexicon\n",
    "review_sample=df.loc[[0,1000,49000]] #Choose some reviews from the cleaned dataset\n",
    "for i, row in review_sample.iterrows(): #Print the review, actual sentiment, and polarity score\n",
    "  print(\"REVIEW: \", row.review)\n",
    "  print(\"Actual Sentiment: \", row.sentiment)\n",
    "  print('Predicted Sentiment polarity: ', afn.score(row.review)) #Get the AFINN polarity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's see how well the AFINN lexicon predicts the actual sentiment of the reviews (it takes a while to run the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = []\n",
    "for i in df['review'].values: #For each review compute the polarity score, and classify it as positive or negative\n",
    "    score = afn.score(i)\n",
    "    if score<=0:\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        preds.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Share of correct sentiment scores\n",
    "print(np.mean([(preds==df.sentiment.values)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "review_sample=df.loc[[0,1000,49000]] #Choose some reviews from the cleaned dataset\n",
    "for i, row in review_sample.iterrows(): #Print the review, actual sentiment, and polarity score\n",
    "  print(\"REVIEW: \", row.review)\n",
    "  print(\"Actual Sentiment: \", row.sentiment)\n",
    "  print('Predicted Sentiment polarity: ', analyser.polarity_scores(row.review)) #Get the VADER polarity score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's see how well the VADER lexicon predicts the actual sentiment of the reviews (it takes a while to run the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in df['review'].values: #For each review compute the polarity score, and classify it as positive or negative\n",
    "    score = analyser.polarity_scores(i)[\"compound\"]\n",
    "    if score<=0:\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        preds.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Share of correct sentiment scores\n",
    "import numpy as np\n",
    "print(np.mean([(preds==df.sentiment.values)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Applications (III/III): Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Topic modelling is the task of assigning topics to unlabelled text documents\n",
    "\n",
    "- Our movie review example:\n",
    "    - Based on the review texts we can assign the movies into movie genres\n",
    "    - We cluster all the reviews that contains similar words\n",
    "        - For example reviews that contain words like 'horror', 'scared', 'shock', 'blood' may be clustered into the same topic: horror movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "We can make the topic modelling with the [Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2)\n",
    "\n",
    "- LDA is an unsupervised machine learning algorithm\n",
    "- Finds groups of words that appear frequently together across several documents\n",
    "    - The groups of words will then be our topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it work in practice?\n",
    "\n",
    "- The LDA algorithm takes a bag of words model as input\n",
    "- It then outputs two things:\n",
    "    - a document to topic matrix (it allocates each document to a topic)\n",
    "    - a word to topic matrix (it allocates each word to a topic\n",
    "- We need to define the number of topics beforehand (the number of topics is a hyperparameter)!\n",
    "    - This is a bit arbitrary\n",
    "    - Try to play around with it and define different number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's see how it works in Python\n",
    "\n",
    "- First we need to make our bag of words:\n",
    "    - For convenience we use the built-in stop-word library in scikit-learn\n",
    "    - We set the maximum document frequency to 10 percent to exclude very common words\n",
    "    - We limit the number of words to 5000 most frequently occuring words\n",
    "        - It limits the dimensionality of the dataset to ease computation\n",
    "        \n",
    "The maximum document frequency and number of words are hyperparameters that you can tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english', max_df=0.1, max_features=5000)\n",
    "bag = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Second we fit our LDA estimator to the bag of words\n",
    "    - We specify the number of topics to 10\n",
    "    - The code may take 5-10 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,random_state=123) #The random_state parameter pass an integer that makes the result reproducible \n",
    "review_topics = lda.fit_transform(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's now print the 5 most important words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_top_words = 5\n",
    "word_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_): #lda.components_ stores a matrix containing the word importance for each topic\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([word_names[i]\n",
    "    for i in topic.argsort()\\\n",
    "        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on the 5 most important words we may identify following topics:\n",
    "\n",
    "1. Action and comedy movies\n",
    "2. Musicals\n",
    "3. War movies\n",
    "4. Reviews somehow related to the quality of acting (not really a movie genre)\n",
    "5. Movies from home\n",
    "6. Teen movies\n",
    "7. Horror movies\n",
    "8. Bad movies\n",
    "9. Feel-good or family movies\n",
    "10. Movies related to series"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Social Data Science: Text as Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "604903271ebee04a1d41a6e5cf13d16aa8edab4556eed9bb8d54d40193dfe553"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
