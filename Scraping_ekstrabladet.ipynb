{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "import requests\n",
    "import random\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pytest\n",
    "import requests\n",
    "#from connector import Connector\n",
    "logfile = 'log.csv'## name your log file.\n",
    "connector = scraping_class.Connector(logfile) # for logging\n",
    "\n",
    "from numpy.random import randint\n",
    "import logging\n",
    "\n",
    "#\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops\n",
      "2 loops\n",
      "3 loops\n",
      "4 loops\n",
      "5 loops\n",
      "6 loops\n",
      "7 loops\n",
      "8 loops\n",
      "9 loops\n",
      "10 loops\n",
      "11 loops\n",
      "12 loops\n",
      "13 loops\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Get the page source after scrolling\u001b[39;00m\n\u001b[1;32m     42\u001b[0m page_source \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mpage_source\n\u001b[0;32m---> 43\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(page_source, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[39m# Save soup\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msearch_html.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed()\n\u001b[1;32m    336\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 478\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mfeed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmarkup)\n\u001b[1;32m    479\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/builder/_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[1;32m    379\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     parser\u001b[39m.\u001b[39mfeed(markup)\n\u001b[1;32m    381\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    382\u001b[0m     \u001b[39m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[39m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[39m# when there's an error in the doctype declaration.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[39mraise\u001b[39;00m ParserRejectedMarkup(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoahead(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/html/parser.py:172\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    170\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_starttag(i)\n\u001b[1;32m    171\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[0;32m--> 172\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_endtag(i)\n\u001b[1;32m    173\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m<!--\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[1;32m    174\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_comment(i)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/html/parser.py:413\u001b[0m, in \u001b[0;36mHTMLParser.parse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_data(rawdata[i:gtpos])\n\u001b[1;32m    411\u001b[0m         \u001b[39mreturn\u001b[39;00m gtpos\n\u001b[0;32m--> 413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_endtag(elem)\n\u001b[1;32m    414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear_cdata_mode()\n\u001b[1;32m    415\u001b[0m \u001b[39mreturn\u001b[39;00m gtpos\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/builder/_htmlparser.py:176\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malready_closed_empty_element\u001b[39m.\u001b[39mremove(name)\n\u001b[1;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\u001b[39m.\u001b[39mhandle_endtag(name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/__init__.py:763\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_endtag\u001b[0;34m(self, name, nsprefix)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpushTag(tag)\n\u001b[1;32m    761\u001b[0m     \u001b[39mreturn\u001b[39;00m tag\n\u001b[0;32m--> 763\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandle_endtag\u001b[39m(\u001b[39mself\u001b[39m, name, nsprefix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    764\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Called by the tree builder when an ending tag is encountered.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \n\u001b[1;32m    766\u001b[0m \u001b[39m    :param name: Name of the tag.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39m    :param nsprefix: Namespace prefix for the tag.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m     \u001b[39m#print(\"End tag: \" + name)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# header\n",
    "headers = {'email': 'qls153@alumni.ku.dk', 'name': 'Nina Frandsen', 'message': 'Students from university of Copenhagen - doing a project on machine learning'}\n",
    "\n",
    "# Define the search term and category\n",
    "search_term = 'a'\n",
    "\n",
    "# Navigate to the search page\n",
    "url = f'https://ekstrabladet.dk/find/?g=true&q={search_term}'\n",
    "\n",
    "# Function to check if the article belongs to the desired category\n",
    "def is_desired_category(article):\n",
    "    card_meta = article.find(class_='card_meta')\n",
    "    if card_meta:\n",
    "        spans = card_meta.find_all('span')\n",
    "        for span in spans:\n",
    "            if 'Dansk politik' in span.get_text():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "cookie = driver.find_element(By.CSS_SELECTOR, '#CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll') #Here we use a CSS selector\n",
    "cookie.click()\n",
    "\n",
    "# Scrape articles\n",
    "loops = 0\n",
    "urls = []\n",
    "\n",
    "while True:\n",
    "    loops += 1\n",
    "    print(f'{loops} loops')\n",
    "    \n",
    "    # Scroll down to load more articles\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Wait for content to load\n",
    "    \n",
    "    # Get the page source after scrolling\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Save soup\n",
    "    with open(\"search_html.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(soup.prettify())  # Save prettified HTML\n",
    "        f.close()\n",
    "    \n",
    "    # Find articles on the page\n",
    "    articles = soup.find_all(class_='headline-list__item')\n",
    "\n",
    "    for article in articles:\n",
    "        if is_desired_category(article):\n",
    "            article_url = article.find('a')['href']\n",
    "            urls.append(article_url)\n",
    "    \n",
    "    # Save every 100 loops\n",
    "    if loops % 100 == 0:\n",
    "        df_urls = pd.DataFrame(data=urls)\n",
    "        df_urls.to_csv(\"urls.csv\", index=False)\n",
    "        print(f'Saved {len(urls)} urls')\n",
    "\n",
    "    # Check if there's a \"Load more\" button and break the loop if not\n",
    "    load_more_button = driver.find_element(By.ID, 'fnStaticSeachShowMore')\n",
    "    if not load_more_button: \n",
    "        print('No more articles to load')\n",
    "        break\n",
    "    else:\n",
    "        load_more_button.click()\n",
    "        time.sleep(2)  # Wait for content to load\n",
    "\n",
    "# Close the Chrome WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37d8266f34dbaa8283094e2501cd3970b4a8b376e59d9d4280da7bd7bc77f1b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
